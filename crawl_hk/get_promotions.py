import json
import requests
from bs4 import BeautifulSoup

def get_clean_text(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # Xo√° c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
        for tag in soup(["script", "style", "meta", "noscript", "head", "link"]):
            tag.decompose()

        # L·∫•y ph·∫ßn th√¢n trang
        body = soup.body
        if not body:
            body = soup  # fallback n·∫øu kh√¥ng c√≥ th·∫ª <body>

        # L·∫•y text, b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = body.get_text(separator="\n", strip=True)

        # L·ªçc b·ªè d√≤ng tr·ªëng
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        cleaned_text = "\n".join(lines)

        return cleaned_text

    except Exception as e:
        return f"‚ùå L·ªói: {e}"

all_promotions = []

with open("/Users/hehe/Documents/VSC/links_click_discovered.json", "r", encoding="utf-8") as f:
    links = json.load(f)

for url in links:  # links l√† danh s√°ch URL ƒë√£ l·ªçc
    print(f"üì• ƒêang x·ª≠ l√Ω: {url}")
    text = get_clean_text(url)
    all_promotions.append({
        "url": url,
        "text": text
    })

# Ghi ra file JSON
with open("clean_promotions.json", "w", encoding="utf-8") as f:
    json.dump(all_promotions, f, ensure_ascii=False, indent=2)

print("üéâ ƒê√£ l∆∞u t·∫•t c·∫£ v√†o clean_promotions.json")
